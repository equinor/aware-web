import os


class Config:
    ignore_alert_list = os.getenv('IGNORE_ALERTS', '').split(',')
    dashboard_header = os.getenv('DASHBOARD_HEADER', 'Events')
    flask_debug = os.getenv('FLASK_DEBUG', True)
    prometheus_api = os.getenv('PROMETHEUS_API', 'http://prometheus-operator-prometheus.monitoring:9090/api/v1/alerts')
    refresh_interval = os.getenv('REFRESH_INTERVAL', 15)
    mockdata2 = r'''{"status":"success","data":{"alerts":[{"labels":{"alertname":"Watchdog","severity":"none"},"annotations":{"message":"This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n"},"state":"firing","activeAt":"2019-04-07T05:52:15.470372903Z","value":1}]}}
    '''
    mock_data = '''
    {"status":"success","data":{"alerts":[{"labels":{"alertname":"TargetDown","job":"apiserver","severity":"warning"},"annotations":{"message":"100% of the apiserver targets are down."},"state":"firing","activeAt":"2019-03-14T01:12:15.470372903Z","value":100},{"labels":{"alertname":"TargetDown","job":"kube-dns","severity":"warning"},"annotations":{"message":"100% of the kube-dns targets are down."},"state":"firing","activeAt":"2019-03-14T01:12:15.470372903Z","value":100},{"labels":{"alertname":"DeadMansSwitch","severity":"none"},"annotations":{"message":"This is a DeadMansSwitch meant to ensure that the entire alerting pipeline is functional."},"state":"firing","activeAt":"2019-03-14T01:12:15.470372903Z","value":1},{"labels":{"alertname":"KubePodCrashLooping","container":"filebeat","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","namespace":"monitoring","pod":"filebeat-hllqh","service":"prometheus-operator-kube-state-metrics","severity":"critical"},"annotations":{"message":"Pod monitoring/filebeat-hllqh (filebeat) is restarting 0.00 times / second.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping"},"state":"firing","activeAt":"2019-03-14T01:14:49.362790506Z","value":0.003448275862068966},{"labels":{"alertname":"KubePodCrashLooping","container":"filebeat","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","namespace":"monitoring","pod":"filebeat-bg5hb","service":"prometheus-operator-kube-state-metrics","severity":"critical"},"annotations":{"message":"Pod monitoring/filebeat-bg5hb (filebeat) is restarting 0.00 times / second.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping"},"state":"firing","activeAt":"2019-03-14T01:15:19.362790506Z","value":0.0022988505747126436},{"labels":{"alertname":"KubePodNotReady","namespace":"kube-system","pod":"oauth2-proxy-ks-6b9dd85456-s6dsg","severity":"critical"},"annotations":{"message":"Pod kube-system/oauth2-proxy-ks-6b9dd85456-s6dsg has been in a non-ready state for longer than an hour.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready"},"state":"firing","activeAt":"2019-03-14T01:12:49.362790506Z","value":1},{"labels":{"alertname":"KubeDeploymentReplicasMismatch","deployment":"oauth2-proxy-ks","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","namespace":"kube-system","pod":"prometheus-operator-kube-state-metrics-54c7c9fd77-m49s4","service":"prometheus-operator-kube-state-metrics","severity":"critical"},"annotations":{"message":"Deployment kube-system/oauth2-proxy-ks has not matched the expected number of replicas for longer than an hour.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch"},"state":"firing","activeAt":"2019-03-14T01:12:49.362790506Z","value":1},{"labels":{"alertname":"KubeDaemonSetRolloutStuck","daemonset":"filebeat","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","namespace":"monitoring","pod":"prometheus-operator-kube-state-metrics-54c7c9fd77-m49s4","service":"prometheus-operator-kube-state-metrics","severity":"critical"},"annotations":{"message":"Only 0% of the desired Pods of DaemonSet monitoring/filebeat are scheduled and ready.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck"},"state":"firing","activeAt":"2019-03-14T01:12:49.362790506Z","value":0},{"labels":{"alertname":"KubeJobCompletion","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","job_name":"elasticsearch-curator-1552611600","namespace":"monitoring","pod":"prometheus-operator-kube-state-metrics-54c7c9fd77-m49s4","service":"prometheus-operator-kube-state-metrics","severity":"warning"},"annotations":{"message":"Job monitoring/elasticsearch-curator-1552611600 is taking more than one hour to complete.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion"},"state":"firing","activeAt":"2019-03-15T01:00:49.362790506Z","value":1},{"labels":{"alertname":"KubeJobFailed","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","job_name":"elasticsearch-curator-1552611600","namespace":"monitoring","pod":"prometheus-operator-kube-state-metrics-54c7c9fd77-m49s4","service":"prometheus-operator-kube-state-metrics","severity":"warning"},"annotations":{"message":"Job monitoring/elasticsearch-curator-1552611600 failed to complete.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed"},"state":"firing","activeAt":"2019-03-15T01:00:49.362790506Z","value":6},{"labels":{"alertname":"CPUThrottlingHigh","container_name":"config-reloader","namespace":"monitoring","pod_name":"alertmanager-prometheus-operator-alertmanager-0","severity":"warning"},"annotations":{"message":"33% throttling of CPU in namespace monitoring for container config-reloader in pod alertmanager-prometheus-operator-alertmanager-0.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh"},"state":"firing","activeAt":"2019-03-15T11:07:34.559334876Z","value":33.333333333333336},{"labels":{"alertname":"CPUThrottlingHigh","container_name":"rules-configmap-reloader","namespace":"monitoring","pod_name":"prometheus-prometheus-operator-prometheus-0","severity":"warning"},"annotations":{"message":"33% throttling of CPU in namespace monitoring for container rules-configmap-reloader in pod prometheus-prometheus-operator-prometheus-0.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh"},"state":"pending","activeAt":"2019-03-15T11:12:34.559334876Z","value":33.333333333333336},{"labels":{"alertname":"CPUThrottlingHigh","container_name":"prometheus-config-reloader","namespace":"monitoring","pod_name":"prometheus-prometheus-operator-prometheus-0","severity":"warning"},"annotations":{"message":"33% throttling of CPU in namespace monitoring for container prometheus-config-reloader in pod prometheus-prometheus-operator-prometheus-0.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh"},"state":"firing","activeAt":"2019-03-15T10:56:34.559334876Z","value":33.333333333333336},{"labels":{"alertname":"CPUThrottlingHigh","container_name":"heapster-nanny","namespace":"kube-system","pod_name":"heapster-5fb7488d97-l5mtl","severity":"warning"},"annotations":{"message":"25% throttling of CPU in namespace kube-system for container heapster-nanny in pod heapster-5fb7488d97-l5mtl.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh"},"state":"pending","activeAt":"2019-03-15T11:25:34.559334876Z","value":25.2},{"labels":{"alertname":"KubeAPIDown","severity":"critical"},"annotations":{"message":"KubeAPI has disappeared from Prometheus target discovery.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown"},"state":"firing","activeAt":"2019-03-14T01:12:24.667662181Z","value":1},{"labels":{"alertname":"KubeControllerManagerDown","severity":"critical"},"annotations":{"message":"KubeControllerManager has disappeared from Prometheus target discovery.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown"},"state":"firing","activeAt":"2019-03-14T01:12:24.667662181Z","value":1},{"labels":{"alertname":"KubeSchedulerDown","severity":"critical"},"annotations":{"message":"KubeScheduler has disappeared from Prometheus target discovery.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeschedulerdown"},"state":"firing","activeAt":"2019-03-14T01:12:24.667662181Z","value":1},{"labels":{"alertname":"TargetDown","job":"apiserver","severity":"warning"},"annotations":{"description":"100% of apiserver targets are down.","summary":"Targets are down"},"state":"firing","activeAt":"2019-03-14T01:12:08.503964295Z","value":100},{"labels":{"alertname":"TargetDown","job":"kube-dns","severity":"warning"},"annotations":{"description":"100% of kube-dns targets are down.","summary":"Targets are down"},"state":"firing","activeAt":"2019-03-14T01:12:08.503964295Z","value":100},{"labels":{"alertname":"DeadMansSwitch","severity":"none"},"annotations":{"description":"This is a DeadMansSwitch meant to ensure that the entire Alerting pipeline is functional.","summary":"Alerting DeadMansSwitch"},"state":"firing","activeAt":"2019-03-14T01:12:08.503964295Z","value":1},{"labels":{"alertname":"CoreDNSDown","severity":"critical"},"annotations":{"message":"CoreDNS has disappeared from Prometheus target discovery.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-corednsdown"},"state":"firing","activeAt":"2019-03-14T01:12:22.392757395Z","value":1},{"labels":{"alertname":"KubeAPIDown","severity":"critical"},"annotations":{"message":"KubeAPI has disappeared from Prometheus target discovery.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown"},"state":"firing","activeAt":"2019-03-14T01:12:22.392757395Z","value":1},{"labels":{"alertname":"KubeControllerManagerDown","severity":"critical"},"annotations":{"message":"KubeControllerManager has disappeared from Prometheus target discovery.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown"},"state":"firing","activeAt":"2019-03-14T01:12:22.392757395Z","value":1},{"labels":{"alertname":"KubeSchedulerDown","severity":"critical"},"annotations":{"message":"KubeScheduler has disappeared from Prometheus target discovery.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeschedulerdown"},"state":"firing","activeAt":"2019-03-14T01:12:22.392757395Z","value":1},{"labels":{"alertname":"KubePodCrashLooping","container":"filebeat","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","namespace":"monitoring","pod":"filebeat-hllqh","service":"prometheus-operator-kube-state-metrics","severity":"critical"},"annotations":{"message":"Pod monitoring/filebeat-hllqh (filebeat) is restarting 1.03 times / 5 minutes.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping"},"state":"firing","activeAt":"2019-03-14T01:14:47.743318242Z","value":1.0344827586206897},{"labels":{"alertname":"KubePodCrashLooping","container":"filebeat","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","namespace":"monitoring","pod":"filebeat-bg5hb","service":"prometheus-operator-kube-state-metrics","severity":"critical"},"annotations":{"message":"Pod monitoring/filebeat-bg5hb (filebeat) is restarting 0.69 times / 5 minutes.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping"},"state":"firing","activeAt":"2019-03-14T01:15:17.743318242Z","value":0.6896551724137931},{"labels":{"alertname":"KubePodNotReady","namespace":"kube-system","pod":"oauth2-proxy-ks-6b9dd85456-s6dsg","severity":"critical"},"annotations":{"message":"Pod kube-system/oauth2-proxy-ks-6b9dd85456-s6dsg has been in a non-ready state for longer than an hour.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready"},"state":"firing","activeAt":"2019-03-14T01:12:47.743318242Z","value":1},{"labels":{"alertname":"KubeDeploymentReplicasMismatch","deployment":"oauth2-proxy-ks","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","namespace":"kube-system","pod":"prometheus-operator-kube-state-metrics-54c7c9fd77-m49s4","service":"prometheus-operator-kube-state-metrics","severity":"critical"},"annotations":{"message":"Deployment kube-system/oauth2-proxy-ks has not matched the expected number of replicas for longer than an hour.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch"},"state":"firing","activeAt":"2019-03-14T01:12:47.743318242Z","value":1},{"labels":{"alertname":"KubeDaemonSetRolloutStuck","daemonset":"filebeat","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","namespace":"monitoring","pod":"prometheus-operator-kube-state-metrics-54c7c9fd77-m49s4","service":"prometheus-operator-kube-state-metrics","severity":"critical"},"annotations":{"message":"Only 0% of the desired Pods of DaemonSet monitoring/filebeat are scheduled and ready.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck"},"state":"firing","activeAt":"2019-03-14T01:12:47.743318242Z","value":0},{"labels":{"alertname":"KubeJobCompletion","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","job_name":"elasticsearch-curator-1552611600","namespace":"monitoring","pod":"prometheus-operator-kube-state-metrics-54c7c9fd77-m49s4","service":"prometheus-operator-kube-state-metrics","severity":"warning"},"annotations":{"message":"Job monitoring/elasticsearch-curator-1552611600 is taking more than one hour to complete.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion"},"state":"firing","activeAt":"2019-03-15T01:00:47.743318242Z","value":1},{"labels":{"alertname":"KubeJobFailed","endpoint":"http","instance":"10.244.1.78:8080","job":"kube-state-metrics","job_name":"elasticsearch-curator-1552611600","namespace":"monitoring","pod":"prometheus-operator-kube-state-metrics-54c7c9fd77-m49s4","service":"prometheus-operator-kube-state-metrics","severity":"warning"},"annotations":{"message":"Job monitoring/elasticsearch-curator-1552611600 failed to complete.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed"},"state":"firing","activeAt":"2019-03-15T01:00:47.743318242Z","value":6}]}}
'''
    mock_data3 = '''
        {"status":"success","data":{"alerts":[{"labels":{"alertname":"MyTest","job":"apiserver","severity":"warning"},"annotations":{"message":"100% of the apiserver targets are down."},"state":"firing","activeAt":"2019-03-14T01:12:15.470372903Z","value":100}]}}
    '''